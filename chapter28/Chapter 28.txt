28.0 Locks
- Locks are put around critical section to ensure that it executes as if it were a single atomic instruction.

28.1 Locks: The Basic Idea

    lock_t mutex; // some globally-allocated lock ’mutex’
    lock(&mutex);
    balance = balance + 1; // critical section
    unlock(&mutex);

- The lock variable holds the state of the lock. It is either available/unlocked/free or acquired/locked/held.
- Exactly one thread holds the lock and is therefore in a critical section.
- Other information could also be stored like which thread holding the lock or a queue for ordering lock acquisition. Those information is hidden from the user of the lock.

- Calling lock() tries to acquire the lock.
    - If the lock is free as no thread holds it, the thread will acquire the lock and enter the critical section. The thread is the owner of the lock then.
    - If another thread calls lock() on the same variable, it will not return while the lock is held by another thread.
    - This way, other threads are prevented from entering the critical section.

- Once unlock() is called, the lock is available again.
    - If not thread is waiting for the lock, the state is simply changed to free.
    - If there are waiting threads, stuck in lock(), one of them will notice or be informed of this change of the lock's state, acquire it and enter the critical section.

- Locks provide minimal amount of control over scheduling to programmers. They help to transform the chaos of traditional OS scheduling into a more controlled activity.

28.2 Pthread Locks
- The POSIX library uses mutex as a name for locks, as it is used to provide mutual exclusion between threads.
    - If one thread is in the critical section, it excludes the others from entering until it has completed the section.

- Different locks can be used to protect different variables.
    - This can increase security.
    - Coarse-grained locking strategy: One big lock is used any time any critical section is accessed.
    - Fine-grained approach: Different data and data structures are protected with different locks, allowing more threads to be in locked code at once.

28.3 Building A Lock
- To build working locks, hardware and OS support is needed.

28.4 Evaluating Locks
- Correctness: A lock's basic task is to provide mutual exclusion.
- Fairness: Does each thread contending for the lock get a fair shot at acquiring it once it is free?
    - Does any thread contending for the lock starve while doing so, thus never obtaining it?
- Performance: How much time overheads are added by using the lock?
    - In the case of contention, when a single thread is running and grabs and releases the lock, what is the overhead of doing so?
    - In the case of multiple threads contending for the lock on a single CPU, are there performance concerns?
    - How does the lock perform when there are multiple CPUs involved and threads on each contending for the lock?

28.5 Controlling Interrupts
- One of the earliest solutions used to provide mutual exclusion was to disable interrupts for critical sections. It was invented for single-processor systems.

    void lock() {
        DisableInterrupts();
    }

    void unlock() {
        EnableInterrupts();
    }

- By turning off interrupts before entering a critical section, it is ensured the code inside the critical section will not be interrupted and executed as if it was atomic.
- When finished, interrupts are re-enabled.

- Pros:
    - Simplicity

- Cons:
    - Any thread must be allowed to perform a privileged operating (turning interrupts on and off) and must be trusted that it is not abused.
        - A greedy program could call lock() at the beginning of its execution and monopolize the processor.
        - An errant or malicious program could call lock() and go into an endless loop. The os would never regain control of the system.
    - This does not work on multiprocessors. If multiple threads are running on different CPUs, and each try to enter the critical section, it does not matter whether interrupts are disabled.
    - Turning off interrupts for extended periods of time can lead to interrupts becoming lost and therefroe serious systems problems.
        - For example, if the CPU missed the fact that a disk device has finished a read request, how will the OS know to wake the process waiting for said read?

- For those reasons, turning off interrupts is only used in limited contexts as a mutual-exclusion primitive.
    - In some cases, an OS itself will use interrupt masking to guarantee atomicity when accessing its own data structures or at least to prevent messy interrupt handling situations from arising.
    - This usage makes sense, as the trust issue disappears inside the OS.

28.6 A Failed Attempt: Just Using Loads/Stores

    typedef struct __lock_t { int flag; } lock_t;

    void init(lock_t *mutex) {
        // 0 -> lock is available, 1 -> held
        mutex->flag = 0;
    }

    void lock(lock_t *mutex) {
        while (mutex->flag == 1) // TEST the flag
            ; // spin-wait (do nothing)
        mutex->flag = 1; // now SET it!
    }

    void unlock(lock_t *mutex) {
        mutex->flag = 0;
    }

- Problems:
    - Correctness:
   
    Thread 1                            Thread 2
    --------                            --------

    call lock()
    while (flag == 1)
    interrupt: switch to thread 2
                                        call lock()
                                        while (flag == 1)
                                        flag = 1;
                                        interrupt: switch to Thread 1
    flag = 1; // set flag to 1 too

    - Performance: The thread to spin-waiting which wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor where the thread the waiter is waiting for cannot even run until a context switch occurs.

28.7 Building Working Spin Locks with Test-And-Set
- As disabling interrupts does not work on multiple processors and because simple approaches using loads and stores do not work, system designers started to invent hardware support for locking.
- Today, all systems provide this support, even for single CPU systems.

- The simplest bit of hardware support is the test-and-set or atomic exchange instruction. In C, it is defined as:
    
    int TestAndSet(int *old_ptr, int new) {
        int old = *old_ptr; // fetch old value at old_ptr
        *old_ptr = new; // store ’new’ into old_ptr
        return old; // return the old value
    }

    - This sequence of operations is performed atomically.

- This is enough to build a simple spin lock:

    typedef struct __lock_t {
        int flag;
    } lock_t;

    void init(lock_t *lock) {
        // 0: lock is available, 1: lock is held
        lock->flag = 0;
    }

    void lock(lock_t *lock) {
        while (TestAndSet(&lock->flag, 1) == 1)
            ; // spin-wait (do nothing)
    }

    void unlock(lock_t *lock) {
        lock->flag = 0;
    }

- Because of the spinning using CPU cycles until the lock becomes available, a preemptive scheduler which interrupts a thread via a timer is required on a single processor.

- Dekker's algorithm: Just use loads and stores for the concurrency problem, assuming they are atomic with respect to each other, which was true on early hardware.
- Peterson's algorithm: Refined Dekker's approach, also just loads and stores are used.
- Both algorithms do not work on modern hardware anymore.

28.8 Evaluating Spin Locks
- Correctness: Mutual exclusion is provided properly.
- Fairness: Spin locks do not provide any fairness guarantees. A thread spinning may spin forever under contentions and starve.
- Performance:
    - On a single CPU, performance overheads can be big. When the thread holding the lock is preempted within a critical section, the scheduler might then run every other threads, each of which tries to acquire the lock for a time slice. CPU cycles are wasted.
    - On multiple CPUs, spin locks work reasonably well if the number of threads roughly equal the number of CPUs. 
        - If two threads content for a lock and the first one grabs it and the other tries to as well, the other will spin. However, presumably with a short critical section, the lock will become avaibale soon.
        - Spinning to wait for a lock held on another processor therefore does not waste many cycles in this case and can be effective.

28.9 Compare-And-Swap

    int CompareAndSwap(int *ptr, int expected, int new) {
        int original = *ptr;
        if (original == expected)
            *ptr = new;
        return original;
    }

- Called compare-and-swap or compare-and-exchange.

- Usage:
    void lock(lock_t *lock) {
        while (CompareAndSwap(&lock->flag, 0, 1) == 1)
            ; // spin
    }

28.10 Load-Linked and Store-Conditional

    int LoadLinked(int *ptr) {
        return *ptr;
    }

    int StoreConditional(int *ptr, int value) {
        if (no update to *ptr since LL to this addr) {
            *ptr = value;
            return 1; // success!
        } else {
            return 0; // failed to update
        }
    }

    void lock(lock_t *lock) {
        while (1) {
            while (LoadLinked(&lock->flag) == 1)
                ; // spin until it’s zero
            if (StoreConditional(&lock->flag, 1) == 1)
                return; // if set-to-1 was success: done
            // otherwise: try again
        }
    }

    void unlock(lock_t *lock) {
        lock->flag = 0;
    }

28.11 Fetch-And-Add

    int FetchAndAdd(int *ptr) {
        int old = *ptr;
        *ptr = old + 1;
        return old;
    }

- With this, a ticket lock can be built. Other than the previous attempts, it ensures progress for all threads:

    typedef struct __lock_t {
        int ticket;
        int turn;
    } lock_t;

    void lock_init(lock_t *lock) {
        lock->ticket = 0;
        lock->turn = 0;
    }

    void lock(lock_t *lock) {
        int myturn = FetchAndAdd(&lock->ticket);
        while (lock->turn != myturn)
            ; // spin
        }

    void unlock(lock_t *lock) {
        lock->turn = lock->turn + 1;
    }

28.12 Too Much Spinning: What Now?
- With N threads contending for a lock, N - 1  time slices may be wasted spinning and waiting for a single thread to release the lock.
- OS support is required as well to develop a lock that doesn't waste time spinning on the CPU.

28.13 A Simple Approach: Just Yield, Baby

    void init() {
        flag = 0;
    }

    void lock() {
        while (TestAndSet(&flag, 1) == 1)
            yield(); // give up the CPU
    }

    void unlock() {
        flag = 0;
    }

- With two threads, this approach works well.
- If there are many threads, like 100, and one thread acquired the lock and is preempted before releasing it, the other 99 will each call lock(), find the lock held, and yield the CPU.
    - Assuming some kind of round-robin scheduler, each of the 99 will execute this run-and-yield pattern before the thread holding the lock gets to run again.
    - This is better than the spinning approach which would waste 99 time slices spinning, but it is costly because of the context switching.
- No starvation is addressed.

28.14 Using Queues: Sleeping Instead Of Spinning
- If the scheduler makes a bad choice with the previous approaches, a thread that runs must either spin waiting for the lock or yield the CPU immediately.
- There must be control over which thread next gets to acquire the lock after the current holder releases it.

- Support provided by Solaris:
    - park(): Puts a calling thread to sleep
    - unpark(threadID): Wakes a particular thread as designated by threadID
    - These two calls can be used in tandem to build a lock that puts a caller to sleep if it tries to acquire a held lock and wakes it when the lock is free.

    typedef struct __lock_t {
        int flag;
        int guard;
        queue_t *q;
    } lock_t;

    void lock_init(lock_t *m) {
        m->flag = 0;
        m->guard = 0;
        queue_init(m->q);
    }

    void lock(lock_t *m) {
        while (TestAndSet(&m->guard, 1) == 1)
            ; //acquire guard lock by spinning
        if (m->flag == 0) {
            m->flag = 1; // lock is acquired
            m->guard = 0;
        } else {
            queue_add(m->q, gettid());
            m->guard = 0;
            park();
        }
    }

    void unlock(lock_t *m) {
        while (TestAndSet(&m->guard, 1) == 1)
            ; //acquire guard lock by spinning
        if (queue_empty(m->q))
            m->flag = 0; // let go of lock; no one wants it
        else
            unpark(queue_remove(m->q)); // hold lock
                                        // (for next thread!)
        m->guard = 0;
    }

- When a thread is interrupted while acquiring or releasing the lock, other threads may be caused to spin-wait for this one to run again. Spin-waiting is therefore not entirely avoided.
- However, the time spent spining is quite limited.

- Wakeup/waiting race: A switch to another thread, just before the call to park(), could lead to problems.
    - For example, if the just-switched-to thread then released the lock. The subsequent park by the first thread would then sleep forever.
    - This problem can be solved with a setpark() instruction, which a thread can use to indicate it is about to park.
    - If it then happens to be interrupted and another calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.

    queue_add(m->q, gettid());
    setpark(); // new code
    m->guard = 0;

- Spin locks should also be avoided as they also have a problem with correctness. The exact problem is priority inversion:
    - Assuming a system has two threads, T2 with a high priority and T1 with a low priority and the scheduler will always run T2 over T1 except when T2 is for example blocked on I/O.
    - Assuming T2 is blocked and T1 runs, grabs a spin lock and enters a critical section. Now T2 becomes unblocked and the CPU scheduler immediately schedules it. T2 now tries to acquire the lock and because it cannot, as T1 holds the lock, it just keeps spinning. T2 will spin forever and the system is hung.

- Just avoiding the use of spin locks does not avoid the problem of inversion.
    - Imagine three threads, T1, T2 and T3, with T3 at the highest priority and T1 at the lowest.
    - Assuming T1 grabs a lock and then T3 starts and as it has a higher priority than T1, starts running immediately. T3 tries to acquire the lock that T1 holds but gets stuck waiting at T1 is still holding it.
    - If T2 starts to run, it will have a higher priority than T1 and is immediately scheduled.
    - T3 is stuck waiting for T1, which may never run now that T2 is running although T3 has the highest priority.

- Solutions:
    - If the problem is just caused by spin locks, they can be avoided.
    - A higher-priority thread waiting for a lower-priority thread can temporarily boost the lower thread's priority. This is known as priority inheritance.
    - Finally, it can be ensured that all threads have the same priority.

28.15 Different OS, Different Support
- Linux provided futext which is similar to Solaris interface but provided more in-kernel functionality.
    - Each futex has a specifc physical memory location associated with it and a per-futex in-kernel queue.

- futex_wait(address, excpected): Puts the calling thread to sleep, assuming the value at the address address is equal to expected. If it is not equal, the call returns immedietaly.
- futex_wake(address) wakes one thread that is waiting on the queue.

    void mutex_lock (int *mutex) {
        int v;
        // Bit 31 was clear, we got the mutex (fastpath)
        if (atomic_bit_test_set (mutex, 31) == 0)
            return;
        atomic_increment (mutex);
        while (1) {
            if (atomic_bit_test_set (mutex, 31) == 0) {
                atomic_decrement (mutex);
                return;
            }
            // Have to waitFirst to make sure futex value
            // we are monitoring is negative (locked).
            v = *mutex;
            if (v >= 0)
                continue;
            futex_wait (mutex, v);
        }
    }

    void mutex_unlock (int *mutex) {
        // Adding 0x80000000 to counter results in 0 if and
        // only if there are not other interested threads
        if (atomic_add_zero (mutex, 0x80000000))
            return;

        // There are other threads waiting for this mutex,
        // wake one of them up.
        futex_wake (mutex);
    }

- A single integer is used to track both whether the lock is held or not and the number oof waiters on the lock.
    - The high bit is used to track whether the lock is held or not. If it is negative, it is held.
    - All the other bits are used to track the number of waiters on the lock.

- With no contentions for the lock and only one thread acquiring or releasing the lock, very little work is done.

28.16 Two-Phase Locks
- A two-phase lock realizes that spinning can be useful, particulary if the lock is about to be released.
- In the first phase, the lock spins for a while, hoping that it can acquire the lock.
- If the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.
- The Linux lock above is a form of such a lock, but it only spins once. A generalization of this could spin in a loop for a fixed amount before using futex support to sleep.

- Two-phase locks are another instance of a hybrid approach, where combining two good ideas may yield a better one.

